%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Example: Project Report
%
% Source: http://www.howtotex.com
%
% Feel free to distribute this example, but please keep the referral
% to howtotex.com
% Date: March 2011 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edit the title below to update the display in My Documents
%\title{Project Report}
%
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}
\usepackage{multirow}
\usepackage{array,diagbox}

%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{UC Berkeley} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Soccer Match Prediction in the Serie A \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont 								\normalsize
        Yannik Pitcan\\[-3pt]		\normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle

\section{Abstract}
I focus on applying statistical learning methodology to predict the results of soccer games in the Italian Serie A. Using past historical data from several years, features are engineered to capture the performance of a team. Support vector machines, logistic regression, and Adaboost are the best performing techniques with regard to predicting the results of future matches. The emphasis is on analyzing the tradeoffs between these methods' performances on our dataset with respect to precision/recall and error rates.

\section{Introduction}
Sports betting is a major industry in the United States and worldwide. With the advent of statistical learning methodology over the past few decades, inroads have been made into sports prediction in order to determine the outcomes of matches. However, most of this analysis has been done for baseball, basketball, and other games and sports where the game is less fluid. As an avid soccer fan, I decided to attempt to predict soccer match outcomes in the Serie A. The choice of the Italian Serie A was due to a few reasons. I believe the Serie A exhibits a parity among the teams that doesn't exist in other leagues. Also the tactical style of play in this league makes one think that past outcomes can influence the result of future games.

\subsection{Related Literature}
Much of the work on this topic is done by bookmakers. Furthermore, they have access to private data which gives them a leg up on competitors. At Stanford however, a few students (Ulmer and Fernandez, Timmaraju) analyzed classifiers such as SVM, random forests, and Hidden Markov Models for the English Premier League. My belief was that one could improve the feature selection methods used to get more accurate predictions on wins. In particular, ranks of teams were not always incorporated in the prior methods.

Another paper that was of relevance was work by Oyvind and Rue in their paper "Predicting and Retrospective Analysis of Soccer Matches in a League." Oyvind and Rue use a Markov Chain Monte Carlo (MCMC) approach to predicting the outcomes of matches. Although my work does not use a Bayesian dynamic linear model, my ideas for feature selection were influenced by both Oyvind and Rue and Ulmer and Fernandez. In particular, this influenced my decision to investigate a time dependent model of exponential weights when creating form and performance metrics for teams.

\subsection{Dataset}
The data used consisted of the results from the $2007$ season to the current $2016$ season. Seasons $2007$ to $2014$ were used for the training set and the $2015$ and $2016$ (ongoing) seasons were used for the test set. Our training dataset consisted of $3420$ games while our test dataset had $320$ games. For every game, the following data was recorded:

\begin{itemize}
\item Home team
\item Away team
\item Result (Win, Loss, or Draw)
\item Goals scored by Home
\item Goals scored by Away
\item Shots taken by Home
\item Shots taken by Away
\item Corners taken by Home
\item Corners taken by Away
\item Home Win Odds
\item Draw Odds
\item Away Win Odds
\end{itemize}

Most of this data taken from http://www.football-data.co.uk/ and parsed via shell scripting. The betting odds were extracted from the Bet365 website.

\section{Methodology}

The main difference between my work and others is with regard to feature selection. I utilize odds ratios from betting agencies along with match statistics for prediction. Also, I apply time dependent weights in order to determine my features. The rest of this section covers more details about the specifics behind feature selection along with the challenges faced along the way.

\subsubsection{Challenges}

The majority of the difficulty came from the relatively inaccessible data for soccer as opposed to basketball and baseball which have troves of information. In order to attain data that includes player statistics per game, I would have to purchase this through Opta. Thus, I had limited data from which I could use for predictions. Most professional companies incorporate individual player statistics in their analyses, and also they have information about injuries as well.

Another obstacle I ran into was that a game like soccer has so many upsets that defy expectations as opposed to baseball or basketball. Many times a team with more skill loses to a weaker team due to luck. This is because there are so few scoring opportunities in a 90 minute match, especially at the upper echelons of the sport. For example, it is rare to have a repeat champion in the Serie A, with the exception of Juventus in the past few seasons.

\subsubsection{Feature Selection}

This was by far the most intensive task in this project because not only were new features necessary but I had to determine which features are relevant. In order to understand what features to engineer, one must ask the following:

\begin{itemize}
\item How can one capture the 'form' of a team?
\item Is the team playing home or away?
\item Has the team been scoring goals often? What is the overall performance outside of match results?
\end{itemize}

The concept of form is highly discussed in related literature. I experimented with two ways of calculating this.

One of my form calculations used time-dependent weights while the other one did not. As I did literature review, I noticed that time-dependency was not factored in when determining a 'form' feature. Thus I decided to give exponential weights to games played in the past.

For both calculations, one maps $\{win, draw, loss\} \to \{1, 0, -1\}$ in order to have a numeric value representing the result trend of a team as of late. $$x_i = \begin{cases} 1&, win \\ 0&, draw \\ -1&, loss \end{cases}.$$

In other words, assuming we use the past $n$ games for match history, the two form features are calculated as follows:

If the game was played $i$ matches earlier, then
\begin{itemize}
\item $$\tau(\{ x_1,\ldots,x_n\}) = Form_{unweighted} = \frac{1}{n} \sum_i x_i$$
\item $$\tau'(\{ x_1,\ldots,x_n\}) = Form_{weighted} = \frac{1}{n} \sum_i x_i e^{-i}$$
\end{itemize}

Similarly, weighted and unweighted averages $\tau$ and $\tau'$ are applied to the statistics of corners, goals, and shots on target taken by the home and away teams to come up with our new features.

To encapsulate the home advantage, we subtract Away values from the corresponding Home values when calculating our features.

\begin{itemize}
\item Home team
\item Away team
\item $\tau$(results in past $n$ matches of home team) - $\tau$(results in past $n$ matches of away team)
\item $\tau$(shots taken in past $n$ matches of home team) - $\tau$(shots taken in past $n$ matches of away team)
\item $\tau$(corners in past $n$ matches of home team) - $\tau$(corners in past $n$ matches of away team)
\item $\tau$(goals scored in past $n$ matches of home team) - $\tau$(goals scored in past $n$ matches of away team)
\item Betting odds for home team winning
\item Betting odds for draw
\item Betting odds for away team winning
\end{itemize}

The determination of $n$ was arbitrary. I used $n=5$ because we thought that was the ideal number of matches representative of the form of a team. The idea behind this attribute is that we can explain using past results how confident the team is currently. For purposes of comparison, $n=7$ was also used.

Another obstacle one faces when looking at the $n$ past matches is that there are games early in the season where there isn't enough history to calculate features. For those matches, I ignored them when creating the training feature set. This approach was also done in the past (Ulmer, Fernandez). 

I believed the inclusion of betting odds would be an indicator of the ranks of the home and away teams so I used these as features as well. In the end, new feature tables were created for both the training and test data.

\subsection{Models}

Support vector machines, Adaboost, and logistic regression were used to train classifiers using $5$-fold cross validation. The implementation of this was done in Matlab using the Classification Learner package.

\subsubsection{Support Vector Machines}

Past literature suggested that SVMs were the best method for out-of-the-box match prediction. Thus I trained SVMs using the following kernels:
\begin{itemize}
\item Linear
\item Quadratic
\item Cubic
\item Gaussian (radial-basis function).
\end{itemize}

Since I was dealing with a multi-classification problem (three categories were home team wins, away team wins, or a draw), I tried both the one-vs-all and one-vs-one approach.

\subsubsection{Adaboost}

The idea behind Adaboost is to aggregate weak learners through iterations on the training data to create a strong classifier. Intuitively, this sounded promising for game data where one can use decision stumps corresponding to features such as the average of goals scored in the past few games. I trained Adaboost classifiers using $30$ decision trees of $20$ maximum splits.

\subsubsection{Logistic Regression}

Along with SVMs and Adaboost, binary logistic regression was implemented as a baseline classifier. I create two features HomeWins and AwayWins that are $1$ if and only if the home team or the away team wins respectively and $0$ otherwise.

\section{Results and Discussion}

The accuracies in the tables are listed as percentages.

\begin{tabular}{ |p{5cm}||p{5cm}|  }
 \hline
 \multicolumn{2}{|c|}{Multi-class Classifiers for $n=5$} \\
 \hline
Classifier & Accuracy \\
 \hline
 Linear SVM   & 50.9 \\
 Gaussian SVM &   51.4 \\
 Linear (one-vs-all) SVM & 47.2 \\
 Gaussian (one-vs-all) SVM    & 50.5 \\
 Adaboost & 52.3 \\
 \hline
\end{tabular}

\begin{tabular}{ |p{5cm}||p{5cm}|  }
 \hline
 \multicolumn{2}{|c|}{Multi-class classifiers for $n=7$} \\
 \hline
Classifier & Accuracy \\
 \hline
 Linear SVM   & 53.0 \\
 Gaussian SVM &   51.0 \\
 Linear (one-vs-all) SVM & 50.0 \\
 Gaussian (one-vs-all) SVM    & 49.5 \\
 Adaboost & 51.5 \\
 \hline
\end{tabular}

\begin{tabular}{ |p{4cm}||p{4cm}|p{4cm}|  }
 \hline
 \multicolumn{3}{|c|}{Binary Classifiers for $n=5$} \\
 \hline
Classifier & Home Accuracy & Away Accuracy \\
 \hline
 Linear SVM   & 57.8 & 69.7 \\
 Gaussian SVM &   67.0 & 72.9 \\
 Adaboost & 72.9 & 68.4 \\
 Logistic Regression & 69.7 & 45.9 \\
 \hline
\end{tabular}

I ran all of our classifiers on the test feature set. SVM with a linear kernel performed the best when looking at $7$ days for feature creation with an accuracy of $53\%$. Adaboost performed similarly with $51.5\%$ success. Overall, there was little difference between performance of SVM and Adaboost. However, the methods predicted better for SVM using a one-vs-one as opposed to a one-vs-all approach. Considering that we would expect 33\% success with randomly guessing, these results were fairly promising.

In the binary classification setting, Adaboost predicted wins at home well with about a $73\%$ accuracy while Gaussian SVM did well guessing away team wins also with $73\%$ accuracy. Of note is that our baseline, logistic regression, performed relatively badly when guessing when the away team won.

Another note of interest is that away wins were underpredicted by both Adaboost and SVM. The linear SVM and Adaboost predicted only $2$ and $3$ out of $60$ away wins correctly respectively. Both performed well when predicting wins at home however.

\begin{table}[!h]
\centering
\begin{tabular}{|l||*{3}{c|}}\hline
\backslashbox{Actual}{Predicted}
&\makebox[3em]{H}&\makebox[3em]{D}&\makebox[3em]{A}\\\hline\hline
H & 80 & 9 & 3\\\hline
D & 37 & 29 & 0\\\hline
A & 46 & 12 & 2\\\hline
\end{tabular}
\caption{Confusion matrix for Linear SVM}

\begin{tabular}{|l||*{3}{c|}}\hline
\backslashbox{Actual}{Predicted}
&\makebox[3em]{H}&\makebox[3em]{D}&\makebox[3em]{A}\\\hline\hline
H & 77 & 11 & 4\\\hline
D & 31 & 34 & 1\\\hline
A & 32 & 25 & 3\\\hline
\end{tabular}
\caption{Confusion matrix for Linear SVM}
\end{table}

Leading pundits and the betting markets predict with accuracy around the 50\% ballpark as well. Expert pundits in the BBC had a 52.6\% accuracy whereas the betting markets averaged 55.3\% in 2013-2014. Albeit they were betting on the English Premier League, but assuming their methods generalized to the Italian League, my predictions are comparable!

The betting odds did not impact predictions very much however as our results weren't significantly better than those in the past (Ulmer, Fernandez, Timmaraju) who didn't factor in these metrics. A future direction for this project would be to take into account the ranks of teams that are playing using a rating such as the ELO score. Another route would be to take data for players from a virtual source. The video game FIFA gives ratings for their players and teams every year. This may lead to improved prediction accuracy with more features to work with.


\begin{thebibliography}{9}
\bibitem{latexcompanion} 
Michel Goossens, Frank Mittelbach, and Alexander Samarin. 
\textit{The \LaTeX\ Companion}. 
Addison-Wesley, Reading, Massachusetts, 1993.
 
\bibitem{rue} 
Rue, Havard, and Oyvind Salvesen, "Prediction and retrospective analysis of soccer matches in a league" Journal of the Royal Statistical Society: Series D (The Statistician) 49.3 (2000): 399-418.
 
\bibitem{gameon2} 
B. Ulmer \& M. Fernandez,
\\\texttt{Predicting Soccer Match Results in the English Premier League}, 2014

\bibitem{gameon} 
A. S. Timmaraju, A. Palnitkar, \& V. Khanna,
\\\texttt{Game ON! Predicting English Premier League Match Outcomes}, 2013
\end{thebibliography}






%%% End document
\end{document}